# Decoder-only-Transformer-Pre-training-
This repository contains a custom implementation of a decoder-only transformer neural network, pre-trained from scratch on a corpus of Shakespearean text, including monologues and dialogues. Unlike large language models (LLMs) that are often fine-tuned and futher optimized (like PPO for GPT), this model focuses solely on pre-training.
